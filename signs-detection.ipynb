{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "! python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar el modelo YOLO11n con el dataset para detección de señales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! yolo detect train model=yolo11n.pt data=\"/home/m4rc/Desktop/vc_final_assignament/Dataset/signs_dataset/data.yaml\" imgsz=640 batch=16 device=0 epochs=200 patience= 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clases de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'forb_ahead', 1: 'forb_left', 2: 'forb_overtake', 3: 'forb_right', 4: 'forb_speed_over_10', 5: 'forb_speed_over_100', 6: 'forb_speed_over_130', 7: 'forb_speed_over_20', 8: 'forb_speed_over_30', 9: 'forb_speed_over_40', 10: 'forb_speed_over_5', 11: 'forb_speed_over_50', 12: 'forb_speed_over_60', 13: 'forb_speed_over_70', 14: 'forb_speed_over_80', 15: 'forb_speed_over_90', 16: 'forb_stopping', 17: 'forb_trucks', 18: 'forb_u_turn', 19: 'forb_weight_over_3.5t', 20: 'forb_weight_over_7.5t', 21: 'info_bus_station', 22: 'info_crosswalk', 23: 'info_highway', 24: 'info_one_way_traffic', 25: 'info_parking', 26: 'info_taxi_parking', 27: 'mand_bike_lane', 28: 'mand_left', 29: 'mand_left_right', 30: 'mand_pass_left', 31: 'mand_pass_left_right', 32: 'mand_pass_right', 33: 'mand_right', 34: 'mand_roundabout', 35: 'mand_straigh_left', 36: 'mand_straight', 37: 'mand_straight_right', 38: 'prio_give_way', 39: 'prio_priority_road', 40: 'prio_stop', 41: 'warn_children', 42: 'warn_construction', 43: 'warn_crosswalk', 44: 'warn_cyclists', 45: 'warn_domestic_animals', 46: 'warn_other_dangers', 47: 'warn_poor_road_surface', 48: 'warn_roundabout', 49: 'warn_slippery_road', 50: 'warn_speed_bumper', 51: 'warn_traffic_light', 52: 'warn_tram', 53: 'warn_two_way_traffic', 54: 'warn_wild_animals', 55: 'forb_no_parking'}\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Carga el modelo\n",
    "model = YOLO(\"/home/m4rc/Desktop/vc_final_assignament/runs/detect/final_signs_train/weights/best.pt\")\n",
    "\n",
    "# Imprime las clases\n",
    "print(model.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Carga el modelo\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Imprime las clases\n",
    "print(model.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección de objetos con yolo11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def main():\n",
    "    # 1. Define las clases permitidas (class_id: nombre)\n",
    "    ALLOWED_CLASSES = {\n",
    "        0: 'person', \n",
    "        1: 'bicycle', \n",
    "        2: 'car', \n",
    "        3: 'motorcycle', \n",
    "        5: 'bus', \n",
    "        6: 'train', \n",
    "        7: 'truck', \n",
    "        8: 'boat', \n",
    "        9: 'traffic light', \n",
    "        11: 'stop sign', \n",
    "        12: 'parking meter', \n",
    "        15: 'cat', \n",
    "        16: 'dog', \n",
    "        17: 'horse',\n",
    "        19: 'cow', \n",
    "        21: 'bear'\n",
    "    }\n",
    "    \n",
    "    # 2. Define un diccionario de colores para cada clase\n",
    "    COLORS = {\n",
    "        0:  (0,   255, 0),   # Verde\n",
    "        1:  (255, 0,   0),   # Azul\n",
    "        2:  (0,   0,   255), # Rojo\n",
    "        3:  (255, 255, 0),   # Cyan\n",
    "        5:  (255, 0,   255), # Magenta\n",
    "        6:  (0,   255, 255), # Amarillo\n",
    "        7:  (128, 128, 0),\n",
    "        8:  (128, 0,   128),\n",
    "        9:  (0,   128, 128),\n",
    "        11: (60,  60,  60),\n",
    "        12: (120, 120, 120),\n",
    "        15: (0,   60,  60),\n",
    "        16: (60,  0,   60),\n",
    "        17: (60,  60,  0),\n",
    "        19: (0,   120, 120),\n",
    "        21: (120, 0,   120)\n",
    "    }\n",
    "\n",
    "    # 3. Carga tu modelo YOLO. Cambia aquí si tu modelo se llama distinto (yolo11n.pt por ejemplo).\n",
    "    model = YOLO('yolo11n.pt')  \n",
    "    \n",
    "    # 4. Abre el video de entrada\n",
    "    video_path = '/home/m4rc/Desktop/vc_final_assignament/media/samples/20241227_103820.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"No se pudo abrir el video.\")\n",
    "        return\n",
    "\n",
    "    # 5. Obtiene propiedades del video original\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Fotogramas por segundo\n",
    "\n",
    "    # 6. Define el códec y crea el objeto VideoWriter para el video de salida\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter('video_salida.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "    # 7. Procesa cada fotograma hasta que el video se acabe o se presione 'q'\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Si no hay más fotogramas, salimos del bucle\n",
    "        \n",
    "        # 8. Detecta objetos en el fotograma (solo con >= 30% de confianza)\n",
    "        results = model.predict(source=frame, conf=0.3)\n",
    "        detections = results[0].boxes\n",
    "\n",
    "        # 9. Dibuja recuadros y etiquetas ÚNICAMENTE para clases permitidas\n",
    "        for det in detections:\n",
    "            x1, y1, x2, y2 = map(int, det.xyxy[0])\n",
    "            class_id = int(det.cls[0])\n",
    "            confidence = float(det.conf[0])\n",
    "\n",
    "            # Solo procesar si la clase está en las permitidas\n",
    "            if class_id in ALLOWED_CLASSES:\n",
    "                color = COLORS.get(class_id, (0, 255, 0))  # color por defecto si no está en el diccionario\n",
    "                class_name = ALLOWED_CLASSES[class_id]\n",
    "                \n",
    "                # Dibujamos el rectángulo\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "                # Escribimos la etiqueta (clase + confianza)\n",
    "                text = f\"{class_name} {confidence:.2f}\"\n",
    "                cv2.putText(frame, text, (x1, max(y1 - 5, 15)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        # 10. Escribe el fotograma anotado en el archivo de salida\n",
    "        out.write(frame)\n",
    "\n",
    "        # (Opcional) Muestra el proceso en una ventana\n",
    "        cv2.imshow('Video Procesado', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 11. Libera recursos\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección aplicando las dos versiones del modeo yolo11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def main():\n",
    "    # 1. Define las clases permitidas (class_id: nombre) para el primer modelo\n",
    "    ALLOWED_CLASSES = {\n",
    "        0: 'person', \n",
    "        1: 'bicycle', \n",
    "        2: 'car', \n",
    "        3: 'motorcycle', \n",
    "        5: 'bus', \n",
    "        6: 'train', \n",
    "        7: 'truck', \n",
    "        8: 'boat', \n",
    "        9: 'traffic light', \n",
    "        11: 'stop sign', \n",
    "        12: 'parking meter', \n",
    "        15: 'cat', \n",
    "        16: 'dog', \n",
    "        17: 'horse',\n",
    "        19: 'cow', \n",
    "        21: 'bear'\n",
    "    }\n",
    "    \n",
    "    # 2. Define un diccionario de colores para las clases del primer modelo\n",
    "    COLORS = {\n",
    "        0:  (0,   255,   0),   # Verde\n",
    "        1:  (255,   0,   0),   # Azul\n",
    "        2:  (0,     0, 255),   # Rojo\n",
    "        3:  (255, 255,   0),   # Cyan\n",
    "        5:  (255,   0, 255),   # Magenta\n",
    "        6:  (0,   255, 255),   # Amarillo\n",
    "        7:  (128, 128,   0),\n",
    "        8:  (128,   0, 128),\n",
    "        9:  (  0, 128, 128),\n",
    "        11: ( 60,  60,  60),\n",
    "        12: (120, 120, 120),\n",
    "        15: (  0,  60,  60),\n",
    "        16: ( 60,   0,  60),\n",
    "        17: ( 60,  60,   0),\n",
    "        19: (  0, 120, 120),\n",
    "        21: (120,   0, 120)\n",
    "    }\n",
    "\n",
    "    # 3. Carga tus dos modelos\n",
    "    model1 = YOLO('yolo11n.pt')  \n",
    "    model2 = YOLO('/home/m4rc/Desktop/vc_final_assignament/runs/detect/train2/weights/best.pt')  \n",
    "    \n",
    "    # 4. Abre el video de entrada\n",
    "    video_path = '/home/m4rc/Desktop/vc_final_assignament/media/samples/20241227_103820.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"No se pudo abrir el video.\")\n",
    "        return\n",
    "\n",
    "    # 5. Obtiene propiedades del video original\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Fotogramas por segundo\n",
    "\n",
    "    # 6. Define el códec y crea el objeto VideoWriter para el video de salida\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter('video_demo.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "    # (Opcional) Color fijo para el segundo modelo (para diferenciarlo claramente)\n",
    "    color_model2 = (255, 255, 255)  # Blanco\n",
    "\n",
    "    # 7. Procesa cada fotograma hasta que el video se acabe o se presione 'q'\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # No hay más fotogramas\n",
    "        \n",
    "        # 8. Detecta objetos con el primer modelo (model1)\n",
    "        results1 = model1.predict(source=frame, conf=0.3)\n",
    "        detections1 = results1[0].boxes\n",
    "        \n",
    "        # 9. Dibuja recuadros y etiquetas ÚNICAMENTE para clases permitidas (model1)\n",
    "        for det in detections1:\n",
    "            x1, y1, x2, y2 = map(int, det.xyxy[0])\n",
    "            class_id = int(det.cls[0])\n",
    "            confidence = float(det.conf[0])\n",
    "\n",
    "            if class_id in ALLOWED_CLASSES:\n",
    "                color = COLORS.get(class_id, (0, 255, 0))  \n",
    "                class_name = ALLOWED_CLASSES[class_id]\n",
    "                \n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                text = f\"[M1] {class_name} {confidence:.2f}\"\n",
    "                cv2.putText(frame, text, (x1, max(y1 - 5, 15)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        # 10. Detecta objetos con el segundo modelo (model2)\n",
    "        results2 = model2.predict(source=frame, conf=0.3)\n",
    "        detections2 = results2[0].boxes\n",
    "\n",
    "        # 11. Dibuja recuadros y etiquetas SIN RESTRICCIONES (model2)\n",
    "        #     Aquí, usamos el nombre que el modelo trae por defecto \n",
    "        #     (results2[0].names[class_id]) y un color fijo (blanco).\n",
    "        for det in detections2:\n",
    "            x1, y1, x2, y2 = map(int, det.xyxy[0])\n",
    "            class_id = int(det.cls[0])\n",
    "            confidence = float(det.conf[0])\n",
    "\n",
    "            # Extraemos el nombre de clase del propio modelo2 si está disponible\n",
    "            # (En YOLOv8, `results[0].names` suele traer un dict de {cls_id: 'nombre'}\n",
    "            #  pero dependiendo de la versión puede variar).\n",
    "            class_name = results2[0].names.get(class_id, str(class_id))\n",
    "            \n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color_model2, 2)\n",
    "            text = f\"[M2] {class_name} {confidence:.2f}\"\n",
    "            cv2.putText(frame, text, (x1, max(y1 - 5, 15)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color_model2, 2)\n",
    "\n",
    "        # 12. Escribe el fotograma anotado en el archivo de salida\n",
    "        out.write(frame)\n",
    "\n",
    "        # (Opcional) Muestra el proceso en una ventana\n",
    "        cv2.imshow('Video Procesado', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 13. Libera recursos\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
